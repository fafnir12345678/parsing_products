from bs4 import BeautifulSoup
import requests
import time
import os

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏, –≥–¥–µ –ª–µ–∂–∏—Ç —ç—Ç–æ—Ç .py —Ñ–∞–π–ª
parsing_dir = os.path.dirname(os.path.abspath(__file__))

url = "https://calorizator.ru/product"
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

res = requests.get(url, headers=headers)
soup = BeautifulSoup(res.text, "lxml")
tables_product = soup.find_all(class_="product") #–≤—Å–µ —Ç–∞–±–ª–∏—Ü—ã —Å –ø—Ä–æ–¥—É–∫—Ç–∞–º–∏ –Ω–∞ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ


prod_link_list=[]
#–∑–∞–ø–æ–ª–Ω—è–µ–º prod_link_list
for table in tables_product:
    products = table.find_all("li")
    for prod in products:
        name_prod = prod.a.get_text().replace('\n', '').replace('             ', '').replace('            ', '')
        link_prod = prod.a.get("href")
        
        #–ø–æ –∏—Ç–æ–≥—É for-–∞ —Å–æ–∑–¥–∞—Å—Ç—Å—è –º–∞—Å—Å–∏–≤ —Å–æ –≤—Å–µ–º–∏ –≥–ª–∞–≤–Ω—ã–º–∏ –ø—Ä–æ–¥—É–∫—Ç–∞–º–∏ –∏ –∏—Ö —Å—Å—ã–ª–∫–∞–º–∏
        prod_link_list.append([name_prod,link_prod]) 
prod_link_list = prod_link_list[:-5:] #—Ç–∞–º –µ—Å—Ç—å —Ç–∞–±–ª–∏—Ü–∞ —Å –ª–∏—á–Ω—ã–º –∫–∞–±–∏–Ω–µ—Ç–æ–º –µ–µ —è —É–±–∏—Ä–∞—é



products_sub_list = []
#—Ä–∞–±–æ—Ç–∞–µ—Ç —Å –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–º prod_link_list –∏ –∑–∞–ø–æ–ª–Ω—è–µ–º products_sub_list
for prod in prod_link_list: 
    name_prod = prod[0]
    mail_linl = f"https://calorizator.ru/{prod[1]}" 
    res_sub = requests.get(mail_linl, headers=headers)
    soup_sub = BeautifulSoup(res_sub.text, "lxml")
    #–∑–∞—Ö–æ–¥–∏–º –≤ –∫–∞–∂–¥—É—é —Å—Å—ã–ª–∫—É –≥–ª–∞–≤–Ω–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞
    products_sub = soup_sub.find("tbody").find_all("tr")


    while True:
        for prod_sub in products_sub:

            #–≤—Å–µ —Å–æ—Å—Ç–∞–≤–Ω—ã–µ —á–∞—Å—Ç–∏ –ø—Ä–æ–¥—É–∫—Ç–∞ (–∏–º—è, –±–µ–ª–∫–∏, –∂–µ—Ä—ã, —É–≥–ª–∏–≤–æ–¥—ã, –∫–∞–ª–ª–æ—Ä–∏–∏)
            prod_sub_el = prod_sub.find_all("td")
            name_sub = prod_sub_el[1].get_text().replace('\n', '').replace('              ', '').replace('             ', '') #–Ω–∞ [0] –∫–∞—Ä—Ç–∏–Ω–∫–∞ –æ–Ω–∞ –º–Ω–µ –Ω–µ –Ω—É–∂–Ω–∞
            protein =prod_sub_el[2].get_text().replace('\n', '').replace('              ', '').replace('            ', '')
            fat = prod_sub_el[3].get_text().replace('\n', '').replace('              ', '').replace('            ', '')
            carb = prod_sub_el[4].get_text().replace('\n', '').replace('              ', '').replace('            ', '')
            kcal = prod_sub_el[5].get_text().replace('\n', '').replace('              ', '').replace('            ', '')
            products_sub_list.append([name_prod, name_sub, protein, fat, carb, kcal])


        #–ø–∞—Ä—Å–∏–º —Ç—É–∂–µ –∫–∞—Ç–µ–≥–æ—Ä–∏—é –Ω–æ —Å–ª–µ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—É
        next_page = soup_sub.find(class_="pager-next last")
        if next_page != None:
            next_page = next_page.a.get("href") #–ø–æ–ª—É—á–∞–µ–º —Å–ª–µ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏–ª–∏ None –µ—Å–ª–∏ –µ–µ –Ω–µ—Ç—É
            mail_linl = f"https://calorizator.ru/{next_page}"  
            res_sub = requests.get(mail_linl, headers=headers)
            soup_sub = BeautifulSoup(res_sub.text, "lxml")
            products_sub = soup_sub.find("tbody").find_all("tr")
            time.sleep(1)
        else:
            break


# –§–æ—Ä–º–∏—Ä—É–µ–º –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –≤–Ω—É—Ç—Ä–∏ parsing6 —Ä—è–¥–æ–º —Å .py
output_file = os.path.join(parsing_dir, "products.txt")

with open(output_file, "w", encoding='utf-8') as f:
    f.write("–ü–†–û–î–£–ö–¢–´ –° –°–ê–ô–¢–ê CALORIZATOR.RU\n")
    f.write("=" * 80 + "\n\n")
    
    for item in products_sub_list:
        # –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã
        name_prod, name_sub, protein, fat, carb, kcal = item
        
        # –ó–∞–ø–∏—Å—å –≤ —Ñ–∞–π–ª
        f.write(f"üìå {name_prod} - {name_sub}\n")
        f.write(f"   –ë–µ–ª–∫–∏: {protein} | –ñ–∏—Ä—ã: {fat} | –£–≥–ª–µ–≤–æ–¥—ã: {carb} | –ö–∞–ª–æ—Ä–∏–∏: {kcal}\n")
        f.write("-" * 80 + "\n")